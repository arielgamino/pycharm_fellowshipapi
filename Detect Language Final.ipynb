{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Attempt on Language Detection\n",
    "\n",
    "This code shows all steps that lead to a model that is run against europar.test file for classification.  It builds on multiple previous attempts.  The code that was used to compare multiple hyperparameters and what this notebook is based on can be found in this file [final_hyperparameter_tuning_using_percentage_of_words.py](final_hyperparameter_tuning_using_percentage_of_words.py)\n",
    "\n",
    "This code uses files in the /pickles directory. If cloning repository from scratch, first the [corpora] (http://www.statmt.org/europarl/) must be downloaded into the /txt folder and the [Create Pickles from Corpora](Create%20Pickles%20from%20Corpora.ipynb) must be run.  Due to space neither the /txt or /pickles files are incorporated into this directory.\n",
    "\n",
    "This notebook shows that a model can be built using three parameters:\n",
    "\n",
    "number_of_documents      - how many documents from corpora to use for training\n",
    "upto_percentage          - percentage of most common words to use per language\n",
    "number_of_common_letters - how many letters of that language to use for feature set creation\n",
    "\n",
    "### Feature set\n",
    "\n",
    "The feature set consist of three parts:\n",
    "\n",
    "common_letters - the most frequent letters to use in a language for classification. Up to n (number_of_common_letters).\n",
    "\n",
    "    features['common_letters_2'] = 'tl'\n",
    "    features['common_letters_3'] = 'tlp'\n",
    "    features['common_letters_4'] = 'tlpo'\n",
    "\n",
    "common_ending - Top n (number_of_common_letters) most common word endings. \n",
    "\n",
    "    features['common_ending_1'] = 'ing'\n",
    "    features['common_ending_2'] = 'ter'\n",
    "    ...\n",
    "    features['common_ending_10'] = 'ong'\n",
    "\n",
    "common_words: top % of words use in each language.*\n",
    "\n",
    "    features['estacion'] = True\n",
    "    features['corriendo'] = False\n",
    "    ...\n",
    "    features['code'] = True\n",
    "    \n",
    "The model with the best performance only uses common_letters and common_ending. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions in the util package\n",
    "# Created to parse and classify the corpora\n",
    "from util.general import *\n",
    "from util.features import *\n",
    "from util.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "number_of_documents      = 5000\n",
    "upto_percentage          = 0\n",
    "number_of_common_letters = 7        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents to extract: 5000\n",
      "Percentage of common words to use:0\n",
      "Elapsed time reading all documents:0:02:55\n",
      "Total Documents:105000\n",
      "Elapsed time reading all words, letters:0:01:09\n",
      "all_documents:105000\n",
      "most_common_words:21\n",
      "bg words:0\n",
      "cs words:0\n",
      "da words:0\n",
      "de words:0\n",
      "el words:0\n",
      "en words:0\n",
      "es words:0\n",
      "et words:0\n",
      "fi words:0\n",
      "fr words:0\n",
      "hu words:0\n",
      "it words:0\n",
      "lt words:0\n",
      "lv words:0\n",
      "nl words:0\n",
      "pl words:0\n",
      "pt words:0\n",
      "ro words:0\n",
      "sk words:0\n",
      "sl words:0\n",
      "sv words:0\n"
     ]
    }
   ],
   "source": [
    "# -------------Step 1-------------\n",
    "# ----READ FROM PICKLE FILES (Pre-processed)----\n",
    "# Get data to create features from corpora\n",
    "pickles_directory = \"pickles\"\n",
    "print(\"Number of documents to extract: \" + str(number_of_documents))\n",
    "print(\"Percentage of common words to use:\" + str(upto_percentage))\n",
    "\n",
    "# Part 1 - Extract documents from corpora\n",
    "start = time.time()\n",
    "all_documents = extract_documents_from_corpora_pickles(pickles_directory, number_of_documents)\n",
    "print(\"Elapsed time reading all documents:\" + print_elapsed_time(start))\n",
    "print(\"Total Documents:\" + str(len(all_documents)))\n",
    "\n",
    "# get common words\n",
    "start = time.time()\n",
    "most_common_words = extract_words_from_corpora_pickles_upto_per(pickles_directory, upto_percentage)\n",
    "elapsed_reading_wl = print_elapsed_time(start)\n",
    "print(\"Elapsed time reading all words, letters:\" + elapsed_reading_wl)\n",
    "print(\"all_documents:\" + str(len(all_documents)))\n",
    "print(\"most_common_words:\" + str(len(most_common_words)))\n",
    "for k, v in most_common_words.items():\n",
    "    print(k+\" words:\"+str(len(v)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_features:0\n",
      "Elapsed time featureset creation:0:08:02\n",
      "featuresets:105000\n"
     ]
    }
   ],
   "source": [
    "# -------------Step 2-------------\n",
    "# Create featureset to be used for training\n",
    "# this is a list of documents with features and label\n",
    "start = time.time()\n",
    "\n",
    "# create word_features\n",
    "word_features = most_common_wordsonly(most_common_words)\n",
    "print(\"words_features:\" + str(len(word_features)))\n",
    "\n",
    "# create featureset\n",
    "featuresets = [(document_features_fromwords(d, word_features, number_of_common_letters), c) for (d, c) in all_documents]\n",
    "elapsed_feature_creation = print_elapsed_time(start)\n",
    "print(\"Elapsed time featureset creation:\" + elapsed_feature_creation)\n",
    "print(\"featuresets:\" + str(len(featuresets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'common_ending_top_1': 'ния',\n",
       "  'common_ending_top_10': 'по',\n",
       "  'common_ending_top_2': 'ата',\n",
       "  'common_ending_top_3': 'и',\n",
       "  'common_ending_top_4': 'чаи',\n",
       "  'common_ending_top_5': 'на',\n",
       "  'common_ending_top_6': 'ята',\n",
       "  'common_ending_top_7': 'ипа',\n",
       "  'common_ending_top_8': 'ава',\n",
       "  'common_ending_top_9': 'ане',\n",
       "  'common_letters_2': 'аи',\n",
       "  'common_letters_3': 'аир',\n",
       "  'common_letters_4': 'аирв',\n",
       "  'common_letters_5': 'аирвн',\n",
       "  'common_letters_6': 'аирвнп',\n",
       "  'common_letters_7': 'аирвнпк'},\n",
       " 'bg')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample of one featureset used for training.\n",
    "featuresets[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:84000\n",
      "Test set:21000\n",
      "Elapsed time for training:0:00:02\n",
      "Classifier Accuracy:96.5904761904762\n",
      "Elapsed time for accuracy testing:0:00:14\n"
     ]
    }
   ],
   "source": [
    "# -------------Step 3-------------\n",
    "# Split train, test \n",
    "# create model with train\n",
    "# score model with test\n",
    "numpy.random.shuffle(featuresets)\n",
    "# calculate how many items to slice by (95% train, 5% test)\n",
    "slice_by = int((80 * len(featuresets)) / 100)\n",
    "train_set, test_set = featuresets[:slice_by], featuresets[slice_by:]\n",
    "print(\"Train set:\" + str(len(train_set)))\n",
    "print(\"Test set:\" + str(len(test_set)))\n",
    "\n",
    "# -------------Step 4-------------\n",
    "# Build the Model\n",
    "start = time.time()\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "elapsed_training = print_elapsed_time(start)\n",
    "print(\"Elapsed time for training:\" + elapsed_training)\n",
    "start = time.time()\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"Classifier Accuracy:\" + str(accuracy * 100))\n",
    "elapsed_accuracy = print_elapsed_time(start)\n",
    "print(\"Elapsed time for accuracy testing:\" + elapsed_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model\n",
    "\n",
    "Once the model has been created, deploy against the europarl.test file.\n",
    "The accuracy of the model is that of those labeled correctly divided by total.\n",
    "\n",
    "The accuracy of this model against the europarl.test files is **92.58**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Total attempted: 21000\n",
      "  Classified correctly: 19442\n",
      "Classified incorrectly: 1558\n",
      "  Europartest Accuracy: 92.58095238095238\n",
      "Elapsed time for accuracy testing:0:00:19\n",
      "Results per language:\n",
      "       bg_correct:999\n",
      "       bg_incorrect:1\n",
      "       cs_incorrect:180\n",
      "       cs_correct:820\n",
      "       da_incorrect:81\n",
      "       da_correct:919\n",
      "       de_correct:912\n",
      "       de_incorrect:88\n",
      "       el_correct:1000\n",
      "       en_correct:934\n",
      "       en_incorrect:66\n",
      "       es_correct:911\n",
      "       es_incorrect:89\n",
      "       et_incorrect:85\n",
      "       et_correct:915\n",
      "       fi_correct:951\n",
      "       fi_incorrect:49\n",
      "       fr_correct:971\n",
      "       fr_incorrect:29\n",
      "       hu_correct:947\n",
      "       hu_incorrect:53\n",
      "       it_incorrect:60\n",
      "       it_correct:940\n",
      "       lt_correct:932\n",
      "       lt_incorrect:68\n",
      "       lv_correct:917\n",
      "       lv_incorrect:83\n",
      "       nl_correct:941\n",
      "       nl_incorrect:59\n",
      "       pl_correct:944\n",
      "       pl_incorrect:56\n",
      "       pt_correct:934\n",
      "       pt_incorrect:66\n",
      "       ro_incorrect:56\n",
      "       ro_correct:944\n",
      "       sk_incorrect:227\n",
      "       sk_correct:773\n",
      "       sl_correct:908\n",
      "       sl_incorrect:92\n",
      "       sv_correct:930\n",
      "       sv_incorrect:70\n"
     ]
    }
   ],
   "source": [
    "# -------------Step 5-------------\n",
    "# Test against europarl_test file\n",
    "\n",
    "europarl_testfile = \"europarl.test\"\n",
    "results_outfile = \"europarl_test_classified_attempt_\" + str(number_of_documents) + \"_\" + str(upto_percentage) + \"_\" + str(number_of_common_letters)+\".csv\"\n",
    "everyother = 20\n",
    "start = time.time()\n",
    "total_ctr, positive_ctr, negative_ctr, language_counter = test_europarltest_file_final(europarl_testfile, classifier, word_features, number_of_common_letters)\n",
    "# results\n",
    "   \n",
    "print(\"       Total attempted: \" + str(total_ctr))\n",
    "print(\"  Classified correctly: \" + str(positive_ctr))\n",
    "print(\"Classified incorrectly: \" + str(negative_ctr))\n",
    "euro_accuracy = (positive_ctr / total_ctr) * 100\n",
    "print(\"  Europartest Accuracy: \"+str(euro_accuracy))\n",
    "elapsed_accuracy = print_elapsed_time(start)\n",
    "print(\"Elapsed time for accuracy testing:\" + elapsed_accuracy)  # Save classifier for deployment\n",
    "\n",
    "print(\"Results per language:\")\n",
    "for k,v in language_counter.items():\n",
    "    print(\"       \"+k+\":\"+str(v))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
