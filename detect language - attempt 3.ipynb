{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Attempt on Language Detection\n",
    "\n",
    "This code shows the third attempt on processing the corpora and trying to come up with a model for the europar.test file.\n",
    "\n",
    "This code uses files in the /pickles directory. This contains the serialized pickle files with processed tokenized documents, all words and all letters in each one of the documents.\n",
    "\n",
    "To build a model, this uses the most common words and most common letters used on each language and builds a feature set around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# These functions were taken from attempt 1 and put together for\n",
    "# easier maintenance and testing.\n",
    "\n",
    "def print_elapsed_time():\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return (\"%d:%02d:%02d\" % (h,m,s))\n",
    "    \n",
    "#Remove any <tags> within text\n",
    "def extract_text_only(text):\n",
    "    soup = BeautifulSoup(text,\"lxml\")\n",
    "#    soup = BeautifulSoup(text,\"html5lib\")\n",
    "    return soup.get_text()    \n",
    "\n",
    "def tokenize_removepuncuation(text):\n",
    "    #words only \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def most_common_words_letter(most_common_words,most_common_letters):\n",
    "    #Create word_features, a list of most common words on all languauges\n",
    "    #this will be used on feature set fed to classifier\n",
    "    word_features = set()\n",
    "    letter_features = set()\n",
    "\n",
    "    for k,v in most_common_words.items():\n",
    "        for word in v:\n",
    "            word_features.add(word[0])\n",
    "  \n",
    "    # Create letter_features, a list of most common letters on all languages\n",
    "    for k,v in most_common_letters.items():\n",
    "        for letter in v:\n",
    "            letter_features.add(letter[0])\n",
    "\n",
    "    return word_features, letter_features\n",
    "\n",
    "#Takes two Counter objects, removes common elements\n",
    "def remove_common_elements(counter1, counter2):\n",
    "    elements_intersect = (counter1 & counter2).most_common()\n",
    "    for n in elements_intersect:\n",
    "        key = n[0]\n",
    "        del counter1[key]\n",
    "        del counter2[key]\n",
    "    return counter1, counter2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature extraction and creation functions\n",
    "\n",
    "#Loop through directory and extract text\n",
    "#directory is the path to directory to process\n",
    "def get_text_from_directory(directory):\n",
    "    language_label = directory.split(\"/\")[-1]    \n",
    "    documents = []\n",
    "    counter = 0\n",
    "    #keep a count on unique words seen on documents\n",
    "    word_counter = Counter()\n",
    "    alphabet_counter = Counter()\n",
    "    alphabet = set()\n",
    "    #Collect a minimun of 5,000 words per directory(language)\n",
    "    for filename in os.listdir(directory):\n",
    "        try:\n",
    "            text_file = open(directory+\"/\"+filename,\"r\").read()\n",
    "            text = extract_text_only(text_file)\n",
    "            #Tokenize words and remove punctuation\n",
    "            tokenized_text = tokenize_removepuncuation(text)\n",
    "            #add to dict counter\n",
    "            word_counter.update(tokenized_text)\n",
    "            #get letters and add to alphabet\n",
    "            [alphabet_counter.update(list(n)) for n in tokenized_text]\n",
    "            documents.append((tokenized_text,language_label))\n",
    "            counter = counter + 1\n",
    "        except:\n",
    "            print(directory+\" - Issue with filename:\"+filename+\" Ignoring.\")\n",
    "    return documents, word_counter, alphabet_counter\n",
    "\n",
    "def extract_data_from_corpora(corpora_directory, number_of_words, number_of_letter, save_pickles):\n",
    "\n",
    "    all_documents = []\n",
    "    most_common_words = {}\n",
    "    most_common_letters = {}\n",
    "\n",
    "    #Loop through all directories contain corpora with all languages\n",
    "    #directory will be the folder containing documents on that language\n",
    "    for directory in os.listdir(corpora_directory):\n",
    "        #full_path contains\n",
    "        full_path = corpora_directory+directory\n",
    "        if(os.path.isdir(full_path)):\n",
    "            print(\"About to process directory \"+directory)\n",
    "            #process directory, text contains documents list with rows (['worda1','worda2,'worda3'],'LANG-A')\n",
    "            #word_counter contains count of all words seen\n",
    "            text, word_counter, alphabet = get_text_from_directory(full_path)\n",
    "            print(\"Number of words for this language:\"+str(len(word_counter)))\n",
    "            #Keep only letters that are not common ascii letters\n",
    "            for letter in list(alphabet):\n",
    "                if(letter in list(string.ascii_letters) or letter in list(string.digits)):\n",
    "                    del alphabet[letter]\n",
    "            #Keep track of most common words per language to use on feature set\n",
    "            most_common_words[directory] = word_counter.most_common(number_of_words)\n",
    "            most_common_letters[directory] = alphabet.most_common(number_of_letter)\n",
    "            if(save_pickles):\n",
    "                #Save to pickle so it can be read without having to process again\n",
    "                pickle_out = open(\"pickles/word_counter_\"+directory+\".pickle\",\"wb\")\n",
    "                pickle.dump(word_counter, pickle_out)\n",
    "                pickle_out.close()\n",
    "                pickle_out = open(\"pickles/alphabet_\"+directory+\".pickle\",\"wb\")\n",
    "                pickle.dump(alphabet, pickle_out)\n",
    "                pickle_out.close()\n",
    "                pickle_out = open(\"pickles/documents_\"+directory+\".pickle\",\"wb\")\n",
    "                pickle.dump(text, pickle_out)\n",
    "                pickle_out.close()\n",
    "\n",
    "            all_documents = all_documents + text\n",
    "        \n",
    "    if(save_pickles):\n",
    "        #Save all_documents to pickle for later use\n",
    "        pickle_out = open(\"pickles/all_documents.pickle\",\"wb\")\n",
    "        pickle.dump(all_documents, pickle_out)\n",
    "        pickle_out.close()\n",
    "    return all_documents, most_common_words, most_common_letters\n",
    "\n",
    "#this function receives a document and creates a feature based list\n",
    "# Input:\n",
    "#   [(['worda1','worda2,'worda3'],'LANG-A'),\n",
    "#     ['wordb1','wordb2,'wordb3'],'LANG-B'),...]\n",
    "# Ouput:\n",
    "# [('αποφασιστικής': False,\n",
    "#   'Περιφερειακής': True,\n",
    "#   'pontot': False,...),'ru'),...]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    #Add word that are part of common words\n",
    "    for word in word_features:\n",
    "        features[word] = (word in document_words)\n",
    "    #Add letters that are part of common letters\n",
    "    #get letters and add to alphabet\n",
    "    document_alphabet = set()\n",
    "    [document_alphabet.update(list(n)) for n in document_words]\n",
    "    #document_alphabet now contains all letters on this document\n",
    "    #Keep only letters that are not common ascii letters\n",
    "    for letter in list(document_alphabet):\n",
    "        if(letter in list(string.ascii_letters) or letter in list(string.digits)):               \n",
    "                document_alphabet.remove(letter)\n",
    "    #Add to feature if exist\n",
    "    for l in letter_features:\n",
    "        features[l] = (l in document_alphabet)\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_data_from_corpora_pickles(pickle_directory, number_of_documents, number_of_words, number_of_letters):\n",
    "    all_documents = []\n",
    "    most_common_words = {}\n",
    "    most_common_letters = {}\n",
    "    \n",
    "    #Read data from pickle files\n",
    "    for filename in os.listdir(pickle_directory):\n",
    "        language = (filename.split('_')[-1]).split('.')[0]\n",
    "        if('word_counter' in filename):\n",
    "            all_words_for_language = pickle.load( open( pickle_directory+\"/\"+filename, \"rb\" ) )\n",
    "            ##eliminate any words that intersect with another language\n",
    "            for lang in most_common_words:\n",
    "                all_words_for_language, most_common_words[lang] = remove_common_elements(all_words_for_language, most_common_words[lang])\n",
    "\n",
    "            most_common_words[language] = all_words_for_language\n",
    "            \n",
    "        elif('alphabet' in filename):\n",
    "            all_letters_for_language = pickle.load( open( pickle_directory+\"/\"+filename, \"rb\" ) )\n",
    "            ##eliminate any words that intersect with another language\n",
    "            for lang in most_common_letters:\n",
    "                all_letters_for_language, most_common_letters[lang] = remove_common_elements(all_letters_for_language, most_common_letters[lang])\n",
    "                \n",
    "            most_common_letters[language] = all_letters_for_language          \n",
    "        elif('documents' in filename):\n",
    "            documents = pickle.load( open( pickle_directory+\"/\"+filename, \"rb\" ) )\n",
    "            doc_limit = number_of_documents if(number_of_documents>=0) else len(all_documents) \n",
    "            all_documents += documents[:doc_limit]\n",
    "        \n",
    "    #return only up to the amount required\n",
    "    for lang in most_common_words:\n",
    "         word_limit   = number_of_words if(number_of_words>=0) else len(most_common_words[lang]) \n",
    "         most_common_words[lang] = most_common_words[lang].most_common(word_limit)\n",
    "\n",
    "    for lang in most_common_letters:\n",
    "         letter_limit   = number_of_letters if(number_of_letters>=0) else len(most_common_letters[lang]) \n",
    "         most_common_letters[lang] = most_common_letters[lang].most_common(letter_limit)\n",
    "    \n",
    "    return all_documents, most_common_words, most_common_letters\n",
    "\n",
    "def extract_documents_from_corpora_pickles(pickle_directory, number_of_documents):\n",
    "    all_documents = []\n",
    "    \n",
    "    #Read data from pickle files\n",
    "    for filename in os.listdir(pickle_directory):\n",
    "        language = (filename.split('_')[-1]).split('.')[0]\n",
    "        if('documents' in filename):\n",
    "            documents = pickle.load( open( pickle_directory+\"/\"+filename, \"rb\" ) )\n",
    "            doc_limit = number_of_documents if(number_of_documents>=0) else len(all_documents) \n",
    "            all_documents += documents[:doc_limit]\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "\n",
    "def extract_wordsletters_from_corpora_pickles(pickle_directory, number_of_words, number_of_letters):\n",
    "    most_common_words = {}\n",
    "    most_common_letters = {}\n",
    "    \n",
    "    #Read data from pickle files\n",
    "    for filename in os.listdir(pickle_directory):\n",
    "        language = (filename.split('_')[-1]).split('.')[0]\n",
    "        if('word_counter' in filename):\n",
    "            all_words_for_language = pickle.load( open( pickle_directory+\"/\"+filename, \"rb\" ) )\n",
    "            ##eliminate any words that intersect with another language\n",
    "            for lang in most_common_words:\n",
    "                all_words_for_language, most_common_words[lang] = remove_common_elements(all_words_for_language, most_common_words[lang])\n",
    "\n",
    "            most_common_words[language] = all_words_for_language\n",
    "            \n",
    "        elif('alphabet' in filename):\n",
    "            all_letters_for_language = pickle.load( open( pickle_directory+\"/\"+filename, \"rb\" ) )\n",
    "            ##eliminate any words that intersect with another language\n",
    "            for lang in most_common_letters:\n",
    "                all_letters_for_language, most_common_letters[lang] = remove_common_elements(all_letters_for_language, most_common_letters[lang])\n",
    "                \n",
    "            most_common_letters[language] = all_letters_for_language          \n",
    "        \n",
    "    #return only up to the amount required\n",
    "    for lang in most_common_words:\n",
    "         word_limit   = number_of_words if(number_of_words>=0) else len(most_common_words[lang]) \n",
    "         most_common_words[lang] = most_common_words[lang].most_common(word_limit)\n",
    "\n",
    "    for lang in most_common_letters:\n",
    "         letter_limit   = number_of_letters if(number_of_letters>=0) else len(most_common_letters[lang]) \n",
    "         most_common_letters[lang] = most_common_letters[lang].most_common(letter_limit)\n",
    "    \n",
    "    return most_common_words, most_common_letters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classification functions\n",
    "\n",
    "def classify_document(document, classifier):\n",
    "    return classifier.classify(document_features(tokenize_removepuncuation(document)))\n",
    "\n",
    "def test_europarltest_file(eurofile, resultsfile, everyother, classifier):\n",
    "    #Read test file and classify each sentence in file\n",
    "    positive_ctr = 0\n",
    "    negative_ctr = 0\n",
    "    total_ctr    = 0 \n",
    "    #save results to file for processing\n",
    "    fileout = open(resultsfile,'w')\n",
    "    #columns\n",
    "    fileout.write('predicted, language given, correctly classified?\\n')\n",
    "    \n",
    "    processed_counter = 0\n",
    "    with open(eurofile,'r') as f:\n",
    "        for line in f:\n",
    "            processed_counter +=1\n",
    "            if(processed_counter%everyother==0):\n",
    "                total_ctr += 1\n",
    "                #language is first two letters in line    \n",
    "                language = line[:2]\n",
    "                #sentence is rest, clean up spaces\n",
    "                sentence = line[2:].strip()\n",
    "                #Detect language based on model\n",
    "                language_detected = classify_document(sentence, classifier)\n",
    "                correctly_classified = language_detected==language\n",
    "                #tally correct and incorrect\n",
    "                if(correctly_classified):\n",
    "                    #correctly classified\n",
    "                    positive_ctr += 1\n",
    "                else:\n",
    "                    #incorrectly classified\n",
    "                    negative_ctr += 1\n",
    "            \n",
    "                fileout.write(language_detected+','+language+','+str(correctly_classified)+'\\n')\n",
    "    fileout.close()\n",
    "    return total_ctr, positive_ctr, negative_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time featureset creation:0:03:39\n",
      "all_documents:21000\n",
      "most_common_words:21\n",
      "most_common_letters:21\n"
     ]
    }
   ],
   "source": [
    "# -------------Step 1-------------\n",
    "# ----READ FROM PICKLE FILES (Pre-processed files)----\n",
    "#Get data to create features from corpora\n",
    "start = time.time()\n",
    "pickles_directory = \"pickles\"\n",
    "#How many documents to process for each language\n",
    "number_of_documents = 1000\n",
    "#how many number of most common words per language\n",
    "number_of_words     = 50 \n",
    "#letters to use per language\n",
    "number_of_letters   = 20\n",
    "\n",
    "all_documents, most_common_words, most_common_letters = extract_data_from_corpora_pickles(pickles_directory, number_of_documents, 100, 100)\n",
    "print(\"Elapsed time featureset creation:\"+print_elapsed_time())\n",
    "print(\"all_documents:\"+str(len(all_documents)))\n",
    "print(\"most_common_words:\"+str(len(most_common_words)))\n",
    "print(\"most_common_letters:\"+str(len(most_common_letters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time reading all documents:0:02:41\n"
     ]
    }
   ],
   "source": [
    "# -------------Step 1-------------\n",
    "# ----READ FROM PICKLE FILES (Pre-read)----\n",
    "#Get data to create features from corpora\n",
    "start = time.time()\n",
    "pickles_directory = \"pickles\"\n",
    "number_of_documents = 3000\n",
    "\n",
    "#Part 1 - get documents\n",
    "start = time.time()\n",
    "all_documents = extract_documents_from_corpora_pickles(pickles_directory,number_of_documents) \n",
    "print(\"Elapsed time reading all documents:\"+print_elapsed_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time reading all words, letters:0:00:32\n",
      "all_documents:66000\n",
      "most_common_words:21\n",
      "most_common_letters:21\n",
      "words_features:14700\n",
      "letter_features:107\n"
     ]
    }
   ],
   "source": [
    "number_of_words     = 700 \n",
    "number_of_letters   = 20\n",
    "#Part 2 - get common words, letters\n",
    "start = time.time()\n",
    "most_common_words, most_common_letters = extract_wordsletters_from_corpora_pickles(pickles_directory, number_of_words, number_of_letters)\n",
    "print(\"Elapsed time reading all words, letters:\"+print_elapsed_time())\n",
    "\n",
    "print(\"all_documents:\"+str(len(all_documents)))\n",
    "print(\"most_common_words:\"+str(len(most_common_words)))\n",
    "print(\"most_common_letters:\"+str(len(most_common_letters)))\n",
    "\n",
    "# -------------Step 2-------------\n",
    "# Create featureset to be used for training\n",
    "# this is a list of documents with features and label\n",
    "start = time.time()\n",
    "\n",
    "#create word_features\n",
    "word_features, letter_features = most_common_words_letter(most_common_words,most_common_letters)\n",
    "print(\"words_features:\"+str(len(word_features)))\n",
    "print(\"letter_features:\"+str(len(letter_features)))\n",
    "\n",
    "#create featureset\n",
    "featuresets = [(document_features(d), c) for (d,c) in all_documents]\n",
    "\n",
    "print(\"Elapsed time featureset creation:\"+print_elapsed_time())\n",
    "print(\"featuresets:\"+str(len(featuresets)))\n",
    "# -------------Step 3-------------\n",
    "# Split train, test for model classification and scoring\n",
    "numpy.random.shuffle(featuresets)\n",
    "#calculate how many items to slice by (95% train, 5% test)\n",
    "slice_by = int((95 * len(featuresets))/100)\n",
    "train_set, test_set =  featuresets[:slice_by], featuresets[slice_by:]\n",
    "print(\"Train set:\"+str(len(train_set)))\n",
    "print(\"Test set:\"+str(len(test_set)))\n",
    "\n",
    "# -------------Step 4-------------\n",
    "# Build the Model\n",
    "start = time.time()\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"Elapsed time for training:\"+print_elapsed_time())\n",
    "# start = time.time()\n",
    "# print(\"Accuracy:\"+str(nltk.classify.accuracy(classifier, test_set)))\n",
    "# print(\"Elapsed time for accuracy testing:\"+print_elapsed_time())\n",
    "\n",
    "#Save classifier for deployment\n",
    "#Save to pickle so it can be tested later with \n",
    "pickle_out = open(\"models/classifier_\"+str(number_of_documents)+\"_\"+str(number_of_words)+\"_\"+str(number_of_letters)+\".pickle\",\"wb\")\n",
    "pickle.dump(classifier, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"models/word_features\"+str(number_of_documents)+\"_\"+str(number_of_words)+\"_\"+str(number_of_letters)+\".pickle\",\"wb\")\n",
    "pickle.dump(word_features, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"models/letter_features\"+str(number_of_documents)+\"_\"+str(number_of_words)+\"_\"+str(number_of_letters)+\".pickle\",\"wb\")\n",
    "pickle.dump(word_features, pickle_out)\n",
    "pickle_out.close()\n",
    "print(\"Done:\"+print_elapsed_time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification of Spanish document:sk\n",
      "Classification of French document:sk\n"
     ]
    }
   ],
   "source": [
    "# -------------Step 5-------------\n",
    "# Test the Model Manually\n",
    "\n",
    "#sample french document\n",
    "fr_document = \"\"\"\n",
    "Madame la Présidente, il s' agit d' une question sensible pour notre Parlement, et plus précisément relative au débat sur l' élargissement. Je veux parler d' une déclaration du ministère turc des Affaires étrangères au sujet du rapport de M. Poos sur l' adhésion de Chypre à l' Union européenne. Dans cette déclaration, le ministère turc des Affaires étrangères porte une attaque inique et diffamatoire contre un député qui a présenté son rapport à la commission compétente. La commission a adopté ce rapport avec une opposition de deux voix seulement. Je crois comprendre que le système parlementaire n' a pas encore atteint en Turquie un niveau de développement tel que ses membres puissent saisir le contenu du rapport concerné et la responsabilité, si tant est que l' on puisse utiliser ce terme, qu' assume M. Poos. Il s' agit d' un rapport du Parlement. En conséquence, je souhaiterais que le Parlement lui-même apporte une réaction à cette attaque injuste.\n",
    "\n",
    "Madame la Présidente, permettez-moi de citer un seul exemple de mauvais goût en lisant le point 1 de la déclaration turque :\n",
    "\"le député est connu, d'une part, pour son opposition à la Turquie et, d'autre part, pour ses liens privés avec l'administration chypriote.\"\n",
    "C' est une pure calomnie, à la limite d' un délit. Je voudrais que le Parlement, ainsi que la Commission, et non M. Poos, adoptent une réaction à cette accusation lancée par la Turquie !\n",
    "Je vous remercie, Madame la Présidente !\n",
    "(Applaudissements)\n",
    "\n",
    "Je vous remercie. Nous allons regarder cela de très près.\n",
    "Il n'y a pas d'autre motion d'ordre.\n",
    "Je dirai simplement à M. Helmer que les fonctionnaires sont actuellement occupés à remettre le drapeau britannique à sa place ; il y avait, en effet, ce matin un problème de drapeau britannique que nous avons tenu à résoudre sans attendre.\n",
    "\n",
    "Situation au Moyen-Orient\n",
    "L'ordre du jour appelle la déclaration du Conseil et de la Commission sur la situation au Moyen-Orient.\n",
    "\n",
    "Madame la Présidente, Monsieur le Commissaire, Mesdames et Messieurs, tout d'abord veuillez excuser mon petit retard, mais je viens en droite ligne de Bruxelles. J'espère que vous voudrez bien le comprendre et je vous en remercie.\n",
    "Mesdames et Messieurs, comme le ministre Louis Michel l'avait déjà indiqué lors de sa comparution, le 28 août dernier, devant la commission des affaires étrangères, des droits de l'homme, de la sécurité commune et de la politique de défense de ce Parlement, on assiste depuis des mois au Proche-Orient à une escalade croissante de la violence, avec pour résultat consternant l'effondrement total de la confiance mutuelle entre les parties et, sur le terrain, cela a créé un profond sentiment d'impuissance parmi toutes les populations concernées.\n",
    "Il ne se passe hélas pas un jour ou à peu près sans que des incidents sanglants et de nouvelles provocations ne se produisent et ne reportent ainsi indéfiniment la matérialisation d'un cessez-le-feu et la fin d'un cycle infernal de représailles, tandis que la liste des victimes ne fait que s'allonger. Cette situation, récemment qualifiée par le ministre Vedrine de monstrueuse et révoltante, suscite bien entendu une profonde inquiétude pour la stabilité d'une région qui est à nos portes. La communauté internationale ne saurait tolérer plus longtemps cette escalade et se doit de condamner avec fermeté les facteurs d'aggravation que constituent notamment le terrorisme et les attentats suicide perpétrés par des Palestiniens en Israël. Outre que ces actes terroristes constituent une abomination, car ils frappent des civils innocents, ils ne font qu'inciter Israël à pratiquer une politique de plus en plus répressive.\n",
    "Les tirs d'activistes palestiniens contre des Israéliens, qu'ils soient colons ou militaires, à partir d'agglomérations sous contrôle palestinien, mais aussi le recours excessif et disproportionné à la force par Israël ne font qu'alimenter le cycle de la violence. L'usage d'avions de combat dans les zones résidentielles, la destruction systématique par des missiles de bâtiments abritant les services de police et de sécurité de l'autorité palestinienne et les meurtres ciblés d'activistes palestiniens ne constituent pas des éléments convaincants d'une stratégie visant à la paix et à la sécurité. Les incursions militaires israéliennes dans les zones passées sous contrôle palestinien sont autant de violations des accords signés. La fermeture des institutions palestiniennes à Jérusalem-Est, et notamment celle de la Maison Orient, et la saisie des archives sont des mesures politiques peu propices à l'apaisement.\n",
    "Madame la Présidente, Mesdames et Messieurs, lorsque nous observons aujourd' hui les perspectives d' une reprise du dialogue direct, où en sommes-nous ? Pour commencer, c' est dans un contexte très inquiétant de radicalisation des différentes parties impliquées dans le conflit que le ministre israélien des Affaires étrangères, M. Shimon Peres, a récemment proposé, en public, la reprise d' un dialogue direct avec l' Autorité palestinienne en vue de réduire la violence et de veiller à ce que le cessez-le-feu soit respecté, deux objectifs qui semblaient inaccessibles jusqu' ici. Au cours d' une récente visite dans la région, le ministre allemand des Affaires étrangères, M. Joschka Fischer, a obtenu que le dirigeant palestinien accepte une telle rencontre entre M. Arafat, donc, d' un côté et M. Shimon Peres de l' autre.\n",
    "\"\"\"\n",
    "\n",
    "#sample spanish document\n",
    "es_document = \"\"\"\n",
    "Nombramiento del Presidente del Banco Central Europeo\n",
    " De conformidad con el orden del día, se procede al debate de la recomendación (A5-0307/2003), en nombre de la Comisión de Asuntos Económicos y Monetarios, relativa al nombramiento del Sr. Jean-Claude Trichet como Presidente del Banco Central Europeo (10893/2003 - C5-0332/2003 - 2003/0819(CNS)) (Ponente: Sra. Randzio-Plath)\n",
    ". (IT) Señor Presidente, Señorías, señores Comisarios, es un gran placer hablar sobre una cuestión de gran importancia para la Unión Europea: el nombramiento del Presidente del Banco Central Europeo.\n",
    "La creación del euro es un éxito considerable en la historia de la integración europea, tanto desde el punto de vista político como técnico. El euro sigue siendo una divisa relativamente nueva, y tendrá que basarse en la experiencia, ampliamente reconocida, del Banco Central Europeo para continuar siendo un éxito.\n",
    "En este momento estamos sustituyendo por primera vez al Presidente del Banco Central Europeo. Es de vital importancia para el Banco Central Europeo y para el euro que la elección de un sucesor del Presidente Duisenberg se lleve a cabo con transparencia y que se base únicamente en los criterios recogidos en el Tratado que establece la Comunidad Europea, así como en el Estatuto del Sistema Europeo de Bancos Centrales y el del Banco Central Europeo, y en el acuerdo de este último sobre el candidato más cualificado para el trabajo.\n",
    "Al nombrar a un sucesor, hemos de reconocer y rendir tributo al excelente trabajo realizado por el Presidente Duisenberg, pero también debemos expresar nuestra confianza en que el Banco Central Europeo seguirá desarrollando todas sus funciones con el mismo grado de éxito que hasta la fecha.\n",
    "La base legal para el procedimiento de nombramiento de un nuevo Presidente del Banco Central Europeo se encuentra en la letra b del apartado 2 del artículo 112, y en apartado 4 del artículo 122, del Tratado que establece la Comunidad Europea, así como en los artículos 11.2 y 43.3 del Protocolo del Estatuto del Sistema Europeo de Bancos Centrales y del Banco Central Europeo.\n",
    "De acuerdo con estas disposiciones, el Consejo de «Información» Ecofin adoptó una recomendación el 15 de julio de 2003 que defendía el nombramiento del Sr. Trichet como Presidente del Banco Central Europeo por un periodo de ocho años con efecto a partir del 1 de noviembre de 2003. La recomendación fue enviada tanto a ustedes como al Banco Central Europeo, de acuerdo con el Tratado, para que pudieran dar su opinión antes de que la recomendación fuera presentada a los Jefes de Estado o de Gobierno para la decisión final.\n",
    "El Consejo de Gobierno del Banco Central Europeo aprobó su opinión el 31 de julio y la envió al Consejo y al Parlamento. Esta opinión confirmó que el Consejo Regulador del Banco Central Europeo cree que el candidato propuesto deber ser una persona de reconocido prestigio y experiencia profesional en materia monetaria y bancaria, como establece la letra b del apartado 2 del artículo 112 del Tratado.\n",
    "Espero que el Parlamento Europeo esté de acuerdo con el Consejo y con el Banco Central Europeo en que el Sr. Trichet es un candidato excelente para este importante puesto. La adopción de la opinión por parte del Parlamento permitirá que los Jefes de Estado o de Gobierno tomen una decisión final sobre la toma de posesión del nuevo Presidente del Banco Central Europeo, dentro del calendario fijado por la recomendación del Consejo.\n",
    "\n",
    "Señor Presidente, en nombre de la Comisión de Asuntos Económicos y Monetarios, recomiendo que el Parlamento confirme el nombramiento del Sr. Jean-Claude Trichet como candidato adecuado para el puesto de Presidente del Banco Central Europeo. El candidato nominado ha presentado una declaración por escrito y ha dado explicaciones orales a dicha comisión en el curso del proceso de confirmación. Ha convencido a los miembros de dicha comisión no solo de su integridad personal y competencia profesional, sino también de sus visión de la política económica y monetaria en la Eurozona. Al mismo tiempo, ha demostrado ser receptivo a las exigencias de una mayor transparencia y responsabilidad democrática en el seno del Banco Central Europeo.\n",
    "Cinco años después de su fundación, el Banco Central Europeo ya es mayor de edad. Su independencia -en términos políticos, económicos, financieros, organizativos y de personal- está garantizada y no está cuestionada por el Tratado que establece la Constitución para Europa. Su alto grado de independencia, que supera al de la Reserva Federal de los Estados Unidos, significa que el BCE tiene una gran responsabilidad en el desarrollo macroeconómico y social. Ello requiere la mayor transparencia posible en interés de la democracia y de la política de integración. Por ello, la transparencia de las decisiones y del procedimiento de toma de decisiones es parte integral del papel del Banco Central Europeo. Este esfuerzo para lograr la transparencia se refleja en el diálogo monetario trimestral con el Parlamento Europeo, en las publicaciones y decisiones ordinarias, pero también en los informes, las conferencias y las previsiones sobre la inflación que se publican cada seis meses. Así pues, en Europa ha tenido lugar una especie de revolución cultural. La cultura de los bancos centrales nacionales en Europa no contaba con este tipo de transparencia. Por cierto, la transparencia también va en interés del BCE porque es todavía una institución nueva, y por lo tanto, se basa especialmente en el establecimiento y la consolidación de su legitimidad, la credibilidad y confianza como autoridad europea.\n",
    "En última instancia, el sistema monetario de una nación refleja todo lo que defiende esa nación y todo a lo que aspira y mantiene, si se me permite citar al renombrado economista europeo Josef Schumpeter. Creo que en la fase actual del debate, la incorporación del Tratado de Maastricht en su totalidad en el borrador del Tratado que establece la Constitución para Europa era el paso más adecuado. A lo largo de los siglos de su existencia, el papel de los bancos centrales ha sufrido un cambio radical, comenzando por su forma de organización privada en la historia, pasando por su cambio de estatus en los Estados Unidos, el concepto de lucha contra la inflación, hasta llegar a su papel de banco emisor independiente. Quizás sea demasiado pronto para encontrar respuesta a los retos actuales. La tarea que debemos emprender es definir el papel del Banco Central en un mundo globalizado dominado por el comercio y los mercados financieros internacionales. Ello implica mercados dinámicos, pero también riesgos cada vez mayores para la estabilidad financiera internacional.\n",
    "¿Qué papel pueden y deben desempeñar, pues, los bancos centrales para contribuir a la estabilidad financiera, evitar las crisis financieras y proporcionar asistencia? ¿Está preparado el BCE para el papel de prestamista en última instancia? ¿Es esto lo que queremos? El euro también ha mejorado la posición internacional de Europa. El BCE tendrá que desempeñar un papel cada vez mayor en la definición y aplicación de las políticas apropiadas para una economía globalizada. Estamos preocupados por el gran desequilibrio de la economía estadounidense y los riesgos que pudiera entrañar para cualquier otra parte del mundo a medio plazo.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Classification of Spanish document:\"+classify_document(es_document,classifier))\n",
    "print(\"Classification of French document:\"+classify_document(fr_document,classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------Step 6-------------\n",
    "# Classify all sentences in europarl.test and write results to resultsfile\n",
    "# This is the actual deployment of the classifier against challenge data\n",
    "europarl_testfile = \"europarl.test\"\n",
    "results_outfile   = \"europarl_test_classified_attempt4.csv\"\n",
    "#use for quick testing, to test just a subset of all documents read\n",
    "#every other 1000 would only classify every 1000 document on testfile\n",
    "everyother = 500\n",
    "start = time.time()\n",
    "total_ctr, positive_ctr, negative_ctr = test_europarltest_file(europarl_testfile, results_outfile, everyother, classifier)\n",
    "#results\n",
    "print(\"       Total attempted: \"+str(total_ctr))\n",
    "print(\"  Classified correctly: \"+str(positive_ctr))\n",
    "print(\"Classified incorrectly: \"+str(negative_ctr))\n",
    "accuracy = (positive_ctr/total_ctr) * 100\n",
    "print(\"              Accuracy: %s\",accuracy)\n",
    "print(\"Elapsed time for accuracy testing:\"+print_elapsed_time())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
