{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pickle files\n",
    "\n",
    "Throughout the code for this repository, pickle files with all the information extracted from txt directory are used. This notebook creates these pickle files by parsing the txt directory and creates these pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# These functions were taken from attempt 1 and put together for\n",
    "# easier maintenance and testing.\n",
    "\n",
    "def print_elapsed_time(start):\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return (\"%d:%02d:%02d\" % (h,m,s))\n",
    "    \n",
    "#Remove any <tags> within text\n",
    "def extract_text_only(text):\n",
    "    soup = BeautifulSoup(text,\"lxml\")\n",
    "#    soup = BeautifulSoup(text,\"html5lib\")\n",
    "    return soup.get_text()    \n",
    "\n",
    "def tokenize_removepuncuation(text):\n",
    "    #words only \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def most_common_words_letter(most_common_words,most_common_letters):\n",
    "    #Create word_features, a list of most common words on all languauges\n",
    "    #this will be used on feature set fed to classifier\n",
    "    word_features = set()\n",
    "    letter_features = set()\n",
    "\n",
    "    for k,v in most_common_words.items():\n",
    "        for word in v:\n",
    "            word_features.add(word[0])\n",
    "  \n",
    "    # Create letter_features, a list of most common letters on all languages\n",
    "    for k,v in most_common_letters.items():\n",
    "        for letter in v:\n",
    "            letter_features.add(letter[0])\n",
    "\n",
    "    return word_features, letter_features\n",
    "\n",
    "#Takes two Counter objects, removes common elements\n",
    "def remove_common_elements(counter1, counter2):\n",
    "    elements_intersect = (counter1 & counter2).most_common()\n",
    "    for n in elements_intersect:\n",
    "        key = n[0]\n",
    "        del counter1[key]\n",
    "        del counter2[key]\n",
    "    return counter1, counter2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature extraction and creation functions\n",
    "\n",
    "#Loop through directory and extract text\n",
    "#directory is the path to directory to process\n",
    "def get_text_from_directory(directory):\n",
    "    language_label = directory.split(\"/\")[-1]    \n",
    "    documents = []\n",
    "    counter = 0\n",
    "    #keep a count on unique words seen on documents\n",
    "    word_counter = Counter()\n",
    "    alphabet_counter = Counter()\n",
    "    alphabet = set()\n",
    "    #Collect a minimun of 5,000 words per directory(language)\n",
    "    for filename in os.listdir(directory):\n",
    "        try:\n",
    "            text_file = open(directory+\"/\"+filename,\"r\").read()\n",
    "            text = extract_text_only(text_file)\n",
    "            #Tokenize words and remove punctuation\n",
    "            tokenized_text = tokenize_removepuncuation(text)\n",
    "            #add to dict counter\n",
    "            word_counter.update(tokenized_text)\n",
    "            #get letters and add to alphabet\n",
    "            [alphabet_counter.update(list(n)) for n in tokenized_text]\n",
    "            documents.append((tokenized_text,language_label))\n",
    "            counter = counter + 1\n",
    "        except:\n",
    "            print(directory+\" - Issue with filename:\"+filename+\" Ignoring.\")\n",
    "    return documents, word_counter, alphabet_counter\n",
    "\n",
    "def extract_data_from_corpora_save_pickles(corpora_directory):\n",
    "\n",
    "    #Loop through all directories contain corpora with all languages\n",
    "    #directory will be the folder containing documents on that language\n",
    "    for directory in os.listdir(corpora_directory):\n",
    "        #full_path contains\n",
    "        full_path = corpora_directory+directory\n",
    "        if(os.path.isdir(full_path)):\n",
    "            print(\"About to process directory \"+directory)\n",
    "            #process directory, text contains documents list with rows (['worda1','worda2,'worda3'],'LANG-A')\n",
    "            #word_counter contains count of all words seen\n",
    "            text, word_counter, alphabet = get_text_from_directory(full_path)\n",
    "            print(\"Number of words for this language:\"+str(len(word_counter)))\n",
    "            \n",
    "            #Keep only letters that are not common ascii letters\n",
    "            for letter in list(alphabet):\n",
    "                if(letter in list(string.ascii_letters) or letter in list(string.digits)):\n",
    "                    del alphabet[letter]\n",
    "\n",
    "\n",
    "            #Save to pickle so it can be read without having to process again\n",
    "            pickle_out = open(\"pickles/word_counter_\"+directory+\".pickle\",\"wb\")\n",
    "            pickle.dump(word_counter, pickle_out)\n",
    "            pickle_out.close()\n",
    "            pickle_out = open(\"pickles/alphabet_\"+directory+\".pickle\",\"wb\")\n",
    "            pickle.dump(alphabet, pickle_out)\n",
    "            pickle_out.close()\n",
    "            pickle_out = open(\"pickles/documents_\"+directory+\".pickle\",\"wb\")\n",
    "            pickle.dump(text, pickle_out)\n",
    "            pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to process directory bg\n",
      "Number of words for this language:107850\n",
      "About to process directory cs\n",
      "Number of words for this language:190273\n",
      "About to process directory da\n",
      "Number of words for this language:350588\n",
      "About to process directory de\n",
      "Number of words for this language:359228\n",
      "About to process directory el\n",
      "Number of words for this language:235146\n",
      "About to process directory en\n",
      "Number of words for this language:101850\n",
      "About to process directory es\n",
      "Number of words for this language:173104\n",
      "About to process directory et\n",
      "Number of words for this language:315598\n",
      "About to process directory fi\n",
      "Number of words for this language:735982\n",
      "About to process directory fr\n",
      "Number of words for this language:131694\n",
      "About to process directory hu\n",
      "Number of words for this language:309740\n",
      "About to process directory it\n",
      "Number of words for this language:178009\n",
      "About to process directory lt\n",
      "Number of words for this language:257164\n",
      "About to process directory lv\n",
      "Number of words for this language:173923\n",
      "About to process directory nl\n",
      "Number of words for this language:255065\n",
      "About to process directory pl\n",
      "txt/pl - Issue with filename:ep-09-10-22-009.txt Ignoring.\n",
      "Number of words for this language:190205\n",
      "About to process directory pt\n",
      "Number of words for this language:162804\n",
      "About to process directory ro\n",
      "Number of words for this language:88194\n",
      "About to process directory sk\n",
      "Number of words for this language:191410\n",
      "About to process directory sl\n",
      "Number of words for this language:156857\n",
      "About to process directory sv\n",
      "Number of words for this language:353605\n",
      "Elapsed time directory creating pickles:1:17:10\n"
     ]
    }
   ],
   "source": [
    "# -----READ CORPORA AND GENERATE PICKLE FILES SYSTEM------\n",
    "corpora_directory = \"txt/\"\n",
    "start = time.time()\n",
    "number_of_words  = 10 \n",
    "number_of_letters = 20\n",
    "\n",
    "save_pickles = True\n",
    "\n",
    "extract_data_from_corpora_save_pickles(corpora_directory)\n",
    "\n",
    "print(\"Elapsed time directory creating pickles:\"+print_elapsed_time(start))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
